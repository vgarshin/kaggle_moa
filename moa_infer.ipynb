{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow.keras.models as M\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras import losses, backend\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "print('tensorflow ver:', tf.__version__)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    for gpu_device in gpu_devices:\n",
    "        print('device available:', gpu_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KAGGLE = False\n",
    "VERS = ['v1', 'v2']\n",
    "PIPES = [1, 2]\n",
    "CUT = 1e-3\n",
    "if KAGGLE:\n",
    "    DATA_PATH = '../input/lish-moa'\n",
    "    MODELS_PATHS = [f'../input/moa-models-{x}' for x in VERS]\n",
    "else:\n",
    "    DATA_PATH = './data'\n",
    "    MODELS_PATHS = [f'./models_{x}' for x in VERS]\n",
    "PARAMS = {}\n",
    "PARAMS['SEED'] = 2022\n",
    "PARAMS['SEEDS'] = 8\n",
    "PARAMS['FOLDS'] = 5\n",
    "PARAMS['EPOCHS'] = 200\n",
    "PARAMS['BATCH_SIZE'] = 32\n",
    "PARAMS['DECAY'] = True\n",
    "PARAMS['PATIENCE'] = 20\n",
    "PARAMS['UNITS'] = 1024\n",
    "PARAMS['DROPOUT'] = .5\n",
    "PARAMS['FEAT_IMP'] = 1\n",
    "PARAMS['PSEUDO_LBL'] = False\n",
    "PARAMS['LBL_SMOOTH'] = 5e-4\n",
    "PARAMS['N_COMPS'] = 48\n",
    "PARAMS['THRESHOLD'] = 1e-2\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv(f'{DATA_PATH}/train_features.csv')\n",
    "train_targets = pd.read_csv(f'{DATA_PATH}/train_targets_scored.csv')\n",
    "test_features = pd.read_csv(f'{DATA_PATH}/test_features.csv')\n",
    "ssubm = pd.read_csv(f'{DATA_PATH}/sample_submission.csv')\n",
    "print(\n",
    "    'train features loaded:', train_features.shape,\n",
    "    '\\ntrain targets loaded:', train_targets.shape,\n",
    "    '\\ntest features loaded:', test_features.shape,\n",
    "    '\\nsubmission loaded:', ssubm.shape,\n",
    ")\n",
    "g_cols = train_features.columns[train_features.columns.str.startswith('g-')]\n",
    "c_cols = train_features.columns[train_features.columns.str.startswith('c-')]\n",
    "feat_cols = g_cols.to_list() + c_cols.to_list()\n",
    "print('g-cols:', len(g_cols), '\\nc-cols:', len(c_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VarianceThreshold:\n",
    "    \n",
    "    def __init__(self, threshold):\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def fit_transform(self, df, feat_cols):\n",
    "        self.df = df\n",
    "        self.var = self.df[feat_cols].var()\n",
    "        self.drop_cols = [x for x in feat_cols \n",
    "                          if x not in self.var[self.var > self.threshold].index.to_list()]\n",
    "        self.valid_cols = [x for x in feat_cols \n",
    "                           if x in self.var[self.var > self.threshold].index.to_list()]\n",
    "        return self.df.drop(self.drop_cols, axis=1), self.valid_cols\n",
    "        \n",
    "    def transform(self, df):\n",
    "        return df.drop(self.drop_cols, axis=1)\n",
    "\n",
    "def preprocess(df):\n",
    "    df = df.copy()\n",
    "    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n",
    "    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n",
    "    del df['sig_id']\n",
    "    df = df.join(pd.get_dummies(df['cp_time'], drop_first=False, prefix='cp_time'))\n",
    "    df = df.drop('cp_time', axis=1)\n",
    "    return df\n",
    "\n",
    "def get_model(num_columns, units=2048, drop=.4, lbl_smooth=.001, pipe=1):\n",
    "    if pipe == 1:\n",
    "        model = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Input(num_columns),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(drop),\n",
    "                tfa.layers.WeightNormalization(\n",
    "                    tf.keras.layers.Dense(units, activation='elu')\n",
    "                ),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(drop),\n",
    "                tfa.layers.WeightNormalization(\n",
    "                    tf.keras.layers.Dense(int(units / 2), activation='elu')\n",
    "                ),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(drop),\n",
    "                tfa.layers.WeightNormalization(\n",
    "                    tf.keras.layers.Dense(206, activation='sigmoid')\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    elif pipe == 2:\n",
    "        model = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Input(num_columns),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(drop),\n",
    "                tfa.layers.WeightNormalization(\n",
    "                    tf.keras.layers.Dense(units, activation='elu')\n",
    "                ),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(drop),\n",
    "                tfa.layers.WeightNormalization(\n",
    "                    tf.keras.layers.Dense(int(units / 2), activation='elu')\n",
    "                ),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(drop),\n",
    "                tfa.layers.WeightNormalization(\n",
    "                    tf.keras.layers.Dense(int(units / 4), activation='elu')\n",
    "                ),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(drop),\n",
    "                tfa.layers.WeightNormalization(\n",
    "                    tf.keras.layers.Dense(206, activation='sigmoid')\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        raise AttributeError('Cannot recover attribute for model pipe')\n",
    "    model.compile(\n",
    "        optimizer=tfa.optimizers.Lookahead(\n",
    "            tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "            sync_period=10\n",
    "        ),\n",
    "        loss=losses.BinaryCrossentropy(label_smoothing=lbl_smooth),\n",
    "        metrics=tf.keras.losses.BinaryCrossentropy(name='score')\n",
    "        #metrics=score\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def metric(y_true, y_pred):\n",
    "    metrics = []\n",
    "    #p_min, p_max = PARAMS['LBL_SMOOTH'], 1 - PARAMS['LBL_SMOOTH']\n",
    "    #y_pred = np.clip(y_pred, p_min, p_max)\n",
    "    for _target in train_targets.columns:\n",
    "        metrics.append(\n",
    "            log_loss(\n",
    "                y_true.loc[:, _target], \n",
    "                y_pred.loc[:, _target].astype(float), \n",
    "                labels=[0, 1]\n",
    "            )\n",
    "        )\n",
    "    return np.mean(metrics)\n",
    "\n",
    "def score(y_true, y_pred):\n",
    "    p_min, p_max = PARAMS['LBL_SMOOTH'], 1 - PARAMS['LBL_SMOOTH']\n",
    "    y_pred = tf.clip_by_value(y_pred, p_min, p_max)\n",
    "    return -backend.mean(\n",
    "        y_true * backend.log(y_pred) + (1 - y_true) * backend.log(1 - y_pred)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic preprocessing\n",
    "train = preprocess(train_features)\n",
    "test = preprocess(test_features)\n",
    "del train_targets['sig_id']\n",
    "train_targets = train_targets.loc[train['cp_type'] == 0].reset_index(drop=True)\n",
    "train = train.loc[train['cp_type'] == 0].reset_index(drop=True)\n",
    "\n",
    "# pca-g feature PCA\n",
    "pca_g = PCA(n_components=PARAMS['N_COMPS'], random_state=PARAMS['SEED'])\n",
    "train_pca = pca_g.fit_transform(train[g_cols])\n",
    "train_pca = pd.DataFrame(\n",
    "train_pca, \n",
    "    columns=[f'pca_g-{i}' for i in range(PARAMS['N_COMPS'])], \n",
    "    index=train.index\n",
    ")\n",
    "train = pd.concat((train, train_pca), axis=1)\n",
    "test_pca = pca_g.transform(test[g_cols])\n",
    "test_pca = pd.DataFrame(\n",
    "    test_pca, \n",
    "    columns=[f'pca_g-{i}' for i in range(PARAMS['N_COMPS'])], \n",
    "    index=test.index\n",
    ")\n",
    "test = pd.concat((test, test_pca), axis=1)\n",
    "feat_cols += [f'pca_g-{i}' for i in range(PARAMS['N_COMPS'])]\n",
    "print('added PCA features:', [f'pca_g-{i}' for i in range(PARAMS['N_COMPS'])])\n",
    "\n",
    "# pca-c feature PCA\n",
    "pca_c = PCA(n_components=int(PARAMS['N_COMPS'] / 8), random_state=PARAMS['SEED'])\n",
    "train_pca = pca_c.fit_transform(train[c_cols])\n",
    "train_pca = pd.DataFrame(\n",
    "    train_pca, \n",
    "    columns=[f'pca_c-{i}' for i in range(int(PARAMS['N_COMPS'] / 8))], \n",
    "    index=train.index\n",
    ")\n",
    "train = pd.concat((train, train_pca), axis=1)\n",
    "test_pca = pca_c.transform(test[c_cols])\n",
    "test_pca = pd.DataFrame(\n",
    "    test_pca, \n",
    "    columns=[f'pca_c-{i}' for i in range(int(PARAMS['N_COMPS'] / 8))], \n",
    "    index=test.index\n",
    ")\n",
    "test = pd.concat((test, test_pca), axis=1)\n",
    "feat_cols += [f'pca_c-{i}' for i in range(int(PARAMS['N_COMPS'] / 8))]\n",
    "print('\\nadded PCA features:', [f'pca_c-{i}' for i in range(int(PARAMS['N_COMPS'] / 8))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "#scaler = StandardScaler()\n",
    "train[feat_cols] = scaler.fit_transform(train[feat_cols])\n",
    "test[feat_cols] = scaler.transform(test[feat_cols])\n",
    "tmp = train.describe().T\n",
    "tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = train[feat_cols].var().sort_values().quantile(PARAMS['THRESHOLD'])\n",
    "print('threshold {:.4f}'.format(threshold))\n",
    "print('features total:', len(feat_cols))\n",
    "var_thresh = VarianceThreshold(threshold)\n",
    "train, feat_cols = var_thresh.fit_transform(train, feat_cols)\n",
    "test = var_thresh.transform(test)\n",
    "print(f'features total with variance threshold {threshold:.4f}:', len(feat_cols))\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f'time elapsed: {elapsed_time // 60:.0f} min {elapsed_time % 60:.0f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssubm.loc[:, train_targets.columns] = 0\n",
    "\n",
    "for i_ver, version in enumerate(VERS):\n",
    "    print('=' * 10, f'version {version}', '=' * 10)\n",
    "    with open(f'{MODELS_PATHS[i_ver]}/top_feats.npy', 'rb') as file:\n",
    "        top_feats = np.load(file)\n",
    "    print('features loaded:', len(top_feats))\n",
    "    all_models = [x for x in os.listdir(MODELS_PATHS[i_ver]) if 'seed_' in x]\n",
    "    \n",
    "    for model_file in all_models:\n",
    "        checkpoint_path = f'{MODELS_PATHS[i_ver]}/{model_file}'\n",
    "        model = get_model(\n",
    "            len(top_feats), \n",
    "            units=PARAMS['UNITS'], \n",
    "            drop=PARAMS['DROPOUT'], \n",
    "            lbl_smooth=PARAMS['LBL_SMOOTH'],\n",
    "            pipe=PIPES[i_ver]\n",
    "        )\n",
    "        model.load_weights(checkpoint_path)\n",
    "        test_predict = model.predict(test.values[:, top_feats], batch_size=PARAMS['BATCH_SIZE'])\n",
    "        ssubm.loc[:, train_targets.columns] += test_predict\n",
    "        print(f'predict for model file  done: {checkpoint_path}')\n",
    "        \n",
    "ssubm.loc[:, train_targets.columns] /= (len(VERS) * PARAMS['FOLDS'] * PARAMS['SEEDS'])\n",
    "\n",
    "ssubm.loc[:, ssubm.columns[1:]] = np.clip(ssubm.loc[:, ssubm.columns[1:]], CUT, 1 - CUT)\n",
    "print('clipped from', np.min(ssubm.min(numeric_only=True)), 'to', max(ssubm.max(numeric_only=True)))\n",
    "ssubm.loc[test['cp_type'] == 1, train_targets.columns] = 0\n",
    "ssubm.to_csv('submission.csv', index=False)\n",
    "print(f'time elapsed: {elapsed_time // 60:.0f} min {elapsed_time % 60:.0f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "s = pd.read_csv('submission.csv')\n",
    "print('s:', np.max(s.loc[:, s.columns[1:]].values), np.min(s.loc[:, s.columns[1:]].values))\n",
    "s_ = pd.read_csv('submission_.csv')\n",
    "print('s_:', np.max(s_.loc[:, s_.columns[1:]].values), np.min(s_.loc[:, s_.columns[1:]].values))\n",
    "s_sc = pd.read_csv('submission_sc.csv')\n",
    "print('s_sc:', np.max(s_sc.loc[:, s_sc.columns[1:]].values), np.min(s_sc.loc[:, s_sc.columns[1:]].values))\n",
    "s_sc2 = pd.read_csv('submission_sc2.csv')\n",
    "print('s_sc2:', np.max(s_sc2.loc[:, s_sc2.columns[1:]].values), np.min(s_sc2.loc[:, s_sc2.columns[1:]].values))\n",
    "s_sc3 = pd.read_csv('submission_sc3.csv')\n",
    "print('s_sc3:', np.max(s_sc3.loc[:, s_sc3.columns[1:]].values), np.min(s_sc3.loc[:, s_sc3.columns[1:]].values))\n",
    "sk = pd.read_csv('submission_kgl.csv')\n",
    "print('sk:', np.max(sk.loc[:, sk.columns[1:]].values), np.min(sk.loc[:, sk.columns[1:]].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4))\n",
    "plt.plot(np.sum(sk.loc[:, sk.columns[1:]].values - s_.loc[:, s_.columns[1:]].values, axis=0), label='sk-s_')\n",
    "plt.plot(np.sum(sk.loc[:, sk.columns[1:]].values - s_sc.loc[:, s_sc.columns[1:]].values, axis=0), label='sk-s_sc')\n",
    "plt.plot(np.sum(sk.loc[:, sk.columns[1:]].values - s_sc2.loc[:, s_sc2.columns[1:]].values, axis=0), label='sk-s_sc2')\n",
    "plt.plot(np.sum(sk.loc[:, sk.columns[1:]].values - s_sc3.loc[:, s_sc3.columns[1:]].values, axis=0), label='sk-s_sc3')\n",
    "plt.plot(np.sum(sk.loc[:, sk.columns[1:]].values - s.loc[:, s.columns[1:]].values, axis=0), label='sk-s')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Orange Python 3",
   "language": "python",
   "name": "orange"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
