{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow.keras.models as M\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras import losses, backend\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, QuantileTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "print('tensorflow ver:', tf.__version__)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    for gpu_device in gpu_devices:\n",
    "        print('device available:', gpu_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KAGGLE = False\n",
    "VERS = ['v40', 'v41', 'v42', 'v43',\n",
    "        'v50', 'v51', 'v52', 'v53']\n",
    "CUT = 1e-3\n",
    "if KAGGLE:\n",
    "    DATA_PATH = '../input/lish-moa'\n",
    "    MODELS_PATHS = [f'../input/moa-models-{x}' for x in VERS]\n",
    "else:\n",
    "    DATA_PATH = './data'\n",
    "    MODELS_PATHS = [f'./models_{x}' for x in VERS]\n",
    "with open(f'{MODELS_PATHS[0]}/params.json') as file:\n",
    "    PARAMS = json.load(file)\n",
    "print('params loaded:', PARAMS)\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_raw = pd.read_csv(f'{DATA_PATH}/train_features.csv')\n",
    "train_targets_raw = pd.read_csv(f'{DATA_PATH}/train_targets_scored.csv')\n",
    "del train_targets_raw['sig_id']\n",
    "test_features_raw = pd.read_csv(f'{DATA_PATH}/test_features.csv')\n",
    "ssubm = pd.read_csv(f'{DATA_PATH}/sample_submission.csv')\n",
    "print(\n",
    "    'train features loaded:', train_features_raw.shape,\n",
    "    '\\ntrain targets loaded:', train_targets_raw.shape,\n",
    "    '\\ntest features loaded:', test_features_raw.shape,\n",
    "    '\\nsubmission loaded:', ssubm.shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VarianceThreshold:\n",
    "    \n",
    "    def __init__(self, threshold):\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def fit_transform(self, df, feat_cols):\n",
    "        self.df = df\n",
    "        self.var = self.df[feat_cols].var()\n",
    "        self.drop_cols = [x for x in feat_cols \n",
    "                          if x not in self.var[self.var > self.threshold].index.to_list()]\n",
    "        self.valid_cols = [x for x in feat_cols \n",
    "                           if x in self.var[self.var > self.threshold].index.to_list()]\n",
    "        return self.df.drop(self.drop_cols, axis=1), self.valid_cols\n",
    "        \n",
    "    def transform(self, df):\n",
    "        return df.drop(self.drop_cols, axis=1)\n",
    "\n",
    "def preprocess(df):\n",
    "    df = df.copy()\n",
    "    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n",
    "    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n",
    "    del df['sig_id']\n",
    "    df = df.join(pd.get_dummies(df['cp_time'], drop_first=False, prefix='cp_time'))\n",
    "    df = df.drop('cp_time', axis=1)\n",
    "    return df \n",
    "    \n",
    "def pca_train_test(train, test, feat_cols, g_cols, c_cols, r_comps, seed):\n",
    "    # pca-g feature PCA\n",
    "    pca_g = PCA(n_components=int(len(g_cols) / r_comps), random_state=seed)\n",
    "    train_pca = pca_g.fit_transform(train[g_cols])\n",
    "    train_pca = pd.DataFrame(\n",
    "        train_pca, \n",
    "        columns=[f'pca_g-{i}' for i in range(int(len(g_cols) / r_comps))], \n",
    "        index=train.index\n",
    "    )\n",
    "    train = pd.concat((train, train_pca), axis=1)\n",
    "    test_pca = pca_g.transform(test[g_cols])\n",
    "    test_pca = pd.DataFrame(\n",
    "        test_pca, \n",
    "        columns=[f'pca_g-{i}' for i in range(int(len(g_cols) / r_comps))], \n",
    "        index=test.index\n",
    "    )\n",
    "    test = pd.concat((test, test_pca), axis=1)\n",
    "    feat_cols += [f'pca_g-{i}' for i in range(int(len(g_cols) / r_comps))]\n",
    "    print('added PCA features:', [f'pca_g-{i}' for i in range(int(len(g_cols) / r_comps))])\n",
    "\n",
    "    # pca-c feature PCA\n",
    "    pca_c = PCA(n_components=int(len(c_cols) / r_comps), random_state=seed)\n",
    "    train_pca = pca_c.fit_transform(train[c_cols])\n",
    "    train_pca = pd.DataFrame(\n",
    "        train_pca, \n",
    "        columns=[f'pca_c-{i}' for i in range(int(len(c_cols) / r_comps))], \n",
    "        index=train.index\n",
    "    )\n",
    "    train = pd.concat((train, train_pca), axis=1)\n",
    "    test_pca = pca_c.transform(test[c_cols])\n",
    "    test_pca = pd.DataFrame(\n",
    "        test_pca, \n",
    "        columns=[f'pca_c-{i}' for i in range(int(len(c_cols) / r_comps))], \n",
    "        index=test.index\n",
    "    )\n",
    "    test = pd.concat((test, test_pca), axis=1)\n",
    "    feat_cols += [f'pca_c-{i}' for i in range(int(len(c_cols) / r_comps))]\n",
    "    print('\\nadded PCA features:', [f'pca_c-{i}' for i in range(int(len(c_cols) / r_comps))])\n",
    "    return train, test, feat_cols\n",
    "\n",
    "def get_feats_stats(df, g_cols, c_cols):\n",
    "    df['stats_g_sum'] = df[g_cols].sum(axis=1)\n",
    "    df['stats_g_mean'] = df[g_cols].mean(axis=1)\n",
    "    df['stats_g_std'] = df[g_cols].std(axis=1)\n",
    "    df['stats_g_kurt'] = df[g_cols].kurtosis(axis=1)\n",
    "    df['stats_g_skew'] = df[g_cols].skew(axis=1)\n",
    "    df['stats_c_sum'] = df[c_cols].sum(axis=1)\n",
    "    df['stats_c_mean'] = df[c_cols].mean(axis=1)\n",
    "    df['stats_c_std'] = df[c_cols].std(axis=1)\n",
    "    df['stats_c_kurt'] = df[c_cols].kurtosis(axis=1)\n",
    "    df['stats_c_skew'] = df[c_cols].skew(axis=1)\n",
    "    df['stats_gc_sum'] = df[g_cols.to_list() + c_cols.to_list()].sum(axis=1)\n",
    "    df['stats_gc_mean'] = df[g_cols.to_list() + c_cols.to_list()].mean(axis=1)\n",
    "    df['stats_gc_std'] = df[g_cols.to_list() + c_cols.to_list()].std(axis=1)\n",
    "    df['stats_gc_kurt'] = df[g_cols.to_list() + c_cols.to_list()].kurtosis(axis=1)\n",
    "    df['stats_gc_skew'] = df[g_cols.to_list() + c_cols.to_list()].skew(axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(train_features, train_targets, test_features,  \n",
    "                   r_comps, q_flag, pipe_scaler, \n",
    "                   seed):\n",
    "    g_cols = train_features.columns[train_features.columns.str.startswith('g-')]\n",
    "    c_cols = train_features.columns[train_features.columns.str.startswith('c-')]\n",
    "    feat_cols = g_cols.to_list() + c_cols.to_list()\n",
    "    print('g-cols:', len(g_cols), '| c-cols:', len(c_cols))\n",
    "    \n",
    "    if q_flag:\n",
    "        qtrans = QuantileTransformer(n_quantiles=100, random_state=seed, output_distribution='normal')\n",
    "        train_features = pd.concat([\n",
    "            train_features.drop(columns=feat_cols), \n",
    "            pd.DataFrame(qtrans.fit_transform(train_features[feat_cols]),\n",
    "                         columns = feat_cols)], axis=1)\n",
    "        test_features = pd.concat([\n",
    "            test_features.drop(columns=feat_cols),\n",
    "            pd.DataFrame(qtrans.transform(test_features[feat_cols]),\n",
    "                         columns = feat_cols)], axis=1)\n",
    "\n",
    "    train = preprocess(train_features)\n",
    "    test = preprocess(test_features)\n",
    "    train_targets = train_targets.loc[train['cp_type'] == 0].reset_index(drop=True)\n",
    "    train = train.loc[train['cp_type'] == 0].reset_index(drop=True)\n",
    "    \n",
    "    train, test, feat_cols = pca_train_test(train, test, feat_cols, \n",
    "                                            g_cols, c_cols,\n",
    "                                            r_comps, seed)\n",
    "    train = get_feats_stats(train, g_cols, c_cols)\n",
    "    test = get_feats_stats(test, g_cols, c_cols)\n",
    "    feat_cols.extend([x for x in train.columns if 'stats_' in x])\n",
    "    print('features:', len(feat_cols))\n",
    "    \n",
    "    if pipe_scaler == 1:\n",
    "        scaler = RobustScaler()\n",
    "    elif pipe_scaler == 2:\n",
    "        scaler = MinMaxScaler()\n",
    "    elif pipe_scaler == 3:\n",
    "        scaler = StandardScaler()\n",
    "    train[feat_cols] = scaler.fit_transform(train[feat_cols])\n",
    "    test[feat_cols] = scaler.transform(test[feat_cols])\n",
    "    \n",
    "    threshold = train[feat_cols].var().sort_values().quantile(PARAMS['THRESHOLD'])\n",
    "    print('threshold {:.4f}'.format(threshold))\n",
    "    print('features total:', len(feat_cols))\n",
    "    var_thresh = VarianceThreshold(threshold)\n",
    "    train, feat_cols = var_thresh.fit_transform(train, feat_cols)\n",
    "    test = var_thresh.transform(test)\n",
    "    print(f'features total with variance threshold {threshold:.4f}:', len(feat_cols))\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f'time elapsed: {elapsed_time // 60:.0f} min {elapsed_time % 60:.0f} sec')\n",
    "    \n",
    "    return train, train_targets, test, feat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_columns, num_columns_rs=0, units=1024, drop=.4, lbl_smooth=.001, pipe=1):\n",
    "    if pipe == 0:\n",
    "        model = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Input(num_columns),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(drop),\n",
    "                tfa.layers.WeightNormalization(\n",
    "                    tf.keras.layers.Dense(units, activation='elu')\n",
    "                ),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(drop),\n",
    "                tfa.layers.WeightNormalization(\n",
    "                    tf.keras.layers.Dense(206, activation='sigmoid')\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    elif pipe == 1:\n",
    "        model = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Input(num_columns),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(drop),\n",
    "                tfa.layers.WeightNormalization(\n",
    "                    tf.keras.layers.Dense(units, activation='elu')\n",
    "                ),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(drop),\n",
    "                tfa.layers.WeightNormalization(\n",
    "                    tf.keras.layers.Dense(int(units / 2), activation='elu')\n",
    "                ),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(drop),\n",
    "                tfa.layers.WeightNormalization(\n",
    "                    tf.keras.layers.Dense(206, activation='sigmoid')\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    elif pipe == 2:\n",
    "        model = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Input(num_columns),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(drop),\n",
    "                tfa.layers.WeightNormalization(\n",
    "                    tf.keras.layers.Dense(units, activation='elu')\n",
    "                ),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(drop),\n",
    "                tfa.layers.WeightNormalization(\n",
    "                    tf.keras.layers.Dense(int(units / 2), activation='elu')\n",
    "                ),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(drop),\n",
    "                tfa.layers.WeightNormalization(\n",
    "                    tf.keras.layers.Dense(int(units / 4), activation='elu')\n",
    "                ),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(drop),\n",
    "                tfa.layers.WeightNormalization(\n",
    "                    tf.keras.layers.Dense(206, activation='sigmoid')\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    elif pipe == 3:\n",
    "        model = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Input(num_columns),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(drop / 2),\n",
    "                tfa.layers.WeightNormalization(\n",
    "                    tf.keras.layers.Dense(int(units / 2), activation='elu')\n",
    "                ),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(drop / 2),\n",
    "                tfa.layers.WeightNormalization(\n",
    "                    tf.keras.layers.Dense(int(units / 2), activation='elu')\n",
    "                ),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(drop / 2),\n",
    "                tfa.layers.WeightNormalization(\n",
    "                    tf.keras.layers.Dense(int(units / 2), activation='elu')\n",
    "                ),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(drop / 2),\n",
    "                tfa.layers.WeightNormalization(\n",
    "                    tf.keras.layers.Dense(int(units / 2), activation='elu')\n",
    "                ),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(drop / 2),\n",
    "                tfa.layers.WeightNormalization(\n",
    "                    tf.keras.layers.Dense(206, activation='sigmoid')\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        raise AttributeError('Cannot recover attribute for model pipe')\n",
    "    model.compile(\n",
    "        optimizer=tfa.optimizers.Lookahead(\n",
    "            tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "            sync_period=10\n",
    "        ),\n",
    "        loss=losses.BinaryCrossentropy(label_smoothing=lbl_smooth),\n",
    "        metrics=tf.keras.losses.BinaryCrossentropy(name='score')\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def metric(y_true, y_pred, smooth=.001):\n",
    "    metrics = []\n",
    "    y_pred = np.clip(y_pred, smooth, 1 - smooth)\n",
    "    for _target in y_true.columns:\n",
    "        metrics.append(\n",
    "            log_loss(\n",
    "                y_true.loc[:, _target], \n",
    "                y_pred.loc[:, _target].astype(float), \n",
    "                labels=[0, 1]\n",
    "            )\n",
    "        )\n",
    "    return np.mean(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssubm.loc[:, train_targets_raw.columns] = 0\n",
    "\n",
    "for i_ver, version in enumerate(VERS):\n",
    "    print('=' * 10, f'version {version}', '=' * 10)\n",
    "    with open(f'{MODELS_PATHS[i_ver]}/params.json') as file:\n",
    "        PARAMS = json.load(file)\n",
    "    print('params loaded:', PARAMS)\n",
    "    train, train_targets, test, _ = get_train_test(\n",
    "        train_features_raw, train_targets_raw, test_features_raw, \n",
    "        r_comps=PARAMS['REDUCE_COMPS'],\n",
    "        q_flag=PARAMS['QTRANS'], pipe_scaler=PARAMS['PIPE_SCALER'],\n",
    "        seed=PARAMS['SEED']\n",
    "    )\n",
    "    print('train done:', train.shape, '| test done:', test.shape)\n",
    "    with open(f'{MODELS_PATHS[i_ver]}/top_feats.npy', 'rb') as file:\n",
    "        top_feats = np.load(file)\n",
    "    print('features loaded:', len(top_feats))\n",
    "    all_models = [x for x in os.listdir(MODELS_PATHS[i_ver]) if 'seed_' in x]\n",
    "    \n",
    "    for model_file in all_models:\n",
    "        checkpoint_path = f'{MODELS_PATHS[i_ver]}/{model_file}'\n",
    "        model = get_model(\n",
    "            len(top_feats), \n",
    "            units=PARAMS['UNITS'], \n",
    "            drop=PARAMS['DROPOUT'], \n",
    "            lbl_smooth=PARAMS['LBL_SMOOTH'],\n",
    "            pipe=PARAMS['PIPE']\n",
    "        )\n",
    "        model.load_weights(checkpoint_path)\n",
    "        test_predict = model.predict(test.values[:, top_feats], batch_size=PARAMS['BATCH_SIZE'])\n",
    "        ssubm.loc[:, train_targets_raw.columns] += test_predict\n",
    "        print(f'predict for model file done: {checkpoint_path}')\n",
    "        \n",
    "ssubm.loc[:, train_targets_raw.columns] /= (len(VERS) * PARAMS['FOLDS'] * PARAMS['SEEDS'])\n",
    "ssubm.loc[:, ssubm.columns[1:]] = np.clip(ssubm.loc[:, ssubm.columns[1:]], CUT, 1 - CUT)\n",
    "print('clipped from', np.min(ssubm.min(numeric_only=True)), 'to', max(ssubm.max(numeric_only=True)))\n",
    "ssubm.loc[test['cp_type'] == 1, train_targets_raw.columns] = 0\n",
    "ssubm.to_csv('submission.csv', index=False)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f'time elapsed: {elapsed_time // 60:.0f} min {elapsed_time % 60:.0f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Orange Python 3",
   "language": "python",
   "name": "orange"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
