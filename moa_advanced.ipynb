{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow.keras.models as M\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras import losses, backend\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, QuantileTransformer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.decomposition import PCA\n",
    "from typing import Tuple, List, Callable, Any\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from hyperopt import hp, tpe, space_eval\n",
    "from hyperopt.fmin import fmin\n",
    "print('tensorflow ver:', tf.__version__)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    for gpu_device in gpu_devices:\n",
    "        print('device available:', gpu_device)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KAGGLE = False\n",
    "VER = 'v53'\n",
    "if KAGGLE:\n",
    "    DATA_PATH = '../input/lish-moa'\n",
    "    MODELS_PATH = f'../input/moa-models-{VER}'\n",
    "else:\n",
    "    DATA_PATH = './data'\n",
    "    MODELS_PATH = f'./models_{VER}'\n",
    "    if not os.path.exists(MODELS_PATH):\n",
    "        os.mkdir(MODELS_PATH)\n",
    "PARAMS = {\n",
    "    'VER': VER,\n",
    "    'SEED': 2053,\n",
    "    'SEEDS': 8,\n",
    "    'FOLDS': 8,\n",
    "    'EPOCHS': 100,\n",
    "    'BATCH_SIZE': 128,\n",
    "    'DECAY': True,\n",
    "    'PATIENCE': 10,\n",
    "    'UNITS': 1024,\n",
    "    'DROPOUT': .5,\n",
    "    'FEAT_IMP': 1, # None or 1, 2, ..., n iterations \n",
    "    'PSEUDO_LBL': False,\n",
    "    'LBL_SMOOTH': 1e-4,\n",
    "    'REDUCE_COMPS': 12,\n",
    "    'THRESHOLD': 1e-2,\n",
    "    'PIPE': 3,\n",
    "    'DATE': str(datetime.now()),\n",
    "    'HOPT': False,\n",
    "    'PIPE_SCALER': 2,\n",
    "    'QTRANS': False\n",
    "}\n",
    "with open(f'{MODELS_PATH}/params.json', 'w') as file:\n",
    "    json.dump(PARAMS, file)\n",
    "\n",
    "def seed_all(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "seed_all(PARAMS['SEED'])\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_raw = pd.read_csv(f'{DATA_PATH}/train_features.csv')\n",
    "train_targets_raw = pd.read_csv(f'{DATA_PATH}/train_targets_scored.csv')\n",
    "del train_targets_raw['sig_id']\n",
    "test_features_raw = pd.read_csv(f'{DATA_PATH}/test_features.csv')\n",
    "ssubm = pd.read_csv(f'{DATA_PATH}/sample_submission.csv')\n",
    "print(\n",
    "    'train features loaded:', train_features_raw.shape,\n",
    "    '\\ntrain targets loaded:', train_targets_raw.shape,\n",
    "    '\\ntest features loaded:', test_features_raw.shape,\n",
    "    '\\nsubmission loaded:', ssubm.shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VarianceThreshold:\n",
    "    \n",
    "    def __init__(self, threshold):\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def fit_transform(self, df, feat_cols):\n",
    "        self.df = df\n",
    "        self.var = self.df[feat_cols].var()\n",
    "        self.drop_cols = [x for x in feat_cols \n",
    "                          if x not in self.var[self.var > self.threshold].index.to_list()]\n",
    "        self.valid_cols = [x for x in feat_cols \n",
    "                           if x in self.var[self.var > self.threshold].index.to_list()]\n",
    "        return self.df.drop(self.drop_cols, axis=1), self.valid_cols\n",
    "        \n",
    "    def transform(self, df):\n",
    "        return df.drop(self.drop_cols, axis=1)\n",
    "\n",
    "def preprocess(df):\n",
    "    df = df.copy()\n",
    "    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n",
    "    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n",
    "    del df['sig_id']\n",
    "    df = df.join(pd.get_dummies(df['cp_time'], drop_first=False, prefix='cp_time'))\n",
    "    df = df.drop('cp_time', axis=1)\n",
    "    return df \n",
    "    \n",
    "def pca_train_test(train, test, feat_cols, g_cols, c_cols, r_comps, seed):\n",
    "    # pca-g feature PCA\n",
    "    pca_g = PCA(n_components=int(len(g_cols) / r_comps), random_state=seed)\n",
    "    train_pca = pca_g.fit_transform(train[g_cols])\n",
    "    train_pca = pd.DataFrame(\n",
    "        train_pca, \n",
    "        columns=[f'pca_g-{i}' for i in range(int(len(g_cols) / r_comps))], \n",
    "        index=train.index\n",
    "    )\n",
    "    train = pd.concat((train, train_pca), axis=1)\n",
    "    test_pca = pca_g.transform(test[g_cols])\n",
    "    test_pca = pd.DataFrame(\n",
    "        test_pca, \n",
    "        columns=[f'pca_g-{i}' for i in range(int(len(g_cols) / r_comps))], \n",
    "        index=test.index\n",
    "    )\n",
    "    test = pd.concat((test, test_pca), axis=1)\n",
    "    feat_cols += [f'pca_g-{i}' for i in range(int(len(g_cols) / r_comps))]\n",
    "    print('added PCA features:', [f'pca_g-{i}' for i in range(int(len(g_cols) / r_comps))])\n",
    "\n",
    "    # pca-c feature PCA\n",
    "    pca_c = PCA(n_components=int(len(c_cols) / r_comps), random_state=seed)\n",
    "    train_pca = pca_c.fit_transform(train[c_cols])\n",
    "    train_pca = pd.DataFrame(\n",
    "        train_pca, \n",
    "        columns=[f'pca_c-{i}' for i in range(int(len(c_cols) / r_comps))], \n",
    "        index=train.index\n",
    "    )\n",
    "    train = pd.concat((train, train_pca), axis=1)\n",
    "    test_pca = pca_c.transform(test[c_cols])\n",
    "    test_pca = pd.DataFrame(\n",
    "        test_pca, \n",
    "        columns=[f'pca_c-{i}' for i in range(int(len(c_cols) / r_comps))], \n",
    "        index=test.index\n",
    "    )\n",
    "    test = pd.concat((test, test_pca), axis=1)\n",
    "    feat_cols += [f'pca_c-{i}' for i in range(int(len(c_cols) / r_comps))]\n",
    "    print('\\nadded PCA features:', [f'pca_c-{i}' for i in range(int(len(c_cols) / r_comps))])\n",
    "    return train, test, feat_cols\n",
    "\n",
    "def get_feats_stats(df, g_cols, c_cols):\n",
    "    df['stats_g_sum'] = df[g_cols].sum(axis=1)\n",
    "    df['stats_g_mean'] = df[g_cols].mean(axis=1)\n",
    "    df['stats_g_std'] = df[g_cols].std(axis=1)\n",
    "    df['stats_g_kurt'] = df[g_cols].kurtosis(axis=1)\n",
    "    df['stats_g_skew'] = df[g_cols].skew(axis=1)\n",
    "    df['stats_c_sum'] = df[c_cols].sum(axis=1)\n",
    "    df['stats_c_mean'] = df[c_cols].mean(axis=1)\n",
    "    df['stats_c_std'] = df[c_cols].std(axis=1)\n",
    "    df['stats_c_kurt'] = df[c_cols].kurtosis(axis=1)\n",
    "    df['stats_c_skew'] = df[c_cols].skew(axis=1)\n",
    "    df['stats_gc_sum'] = df[g_cols.to_list() + c_cols.to_list()].sum(axis=1)\n",
    "    df['stats_gc_mean'] = df[g_cols.to_list() + c_cols.to_list()].mean(axis=1)\n",
    "    df['stats_gc_std'] = df[g_cols.to_list() + c_cols.to_list()].std(axis=1)\n",
    "    df['stats_gc_kurt'] = df[g_cols.to_list() + c_cols.to_list()].kurtosis(axis=1)\n",
    "    df['stats_gc_skew'] = df[g_cols.to_list() + c_cols.to_list()].skew(axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(train_features, train_targets, test_features,  \n",
    "                   r_comps, q_flag, pipe_scaler, \n",
    "                   seed):\n",
    "    g_cols = train_features.columns[train_features.columns.str.startswith('g-')]\n",
    "    c_cols = train_features.columns[train_features.columns.str.startswith('c-')]\n",
    "    feat_cols = g_cols.to_list() + c_cols.to_list()\n",
    "    print('g-cols:', len(g_cols), '| c-cols:', len(c_cols))\n",
    "    \n",
    "    if q_flag:\n",
    "        qtrans = QuantileTransformer(n_quantiles=100, random_state=seed, output_distribution='normal')\n",
    "        train_features = pd.concat([\n",
    "            train_features.drop(columns=feat_cols), \n",
    "            pd.DataFrame(qtrans.fit_transform(train_features[feat_cols]),\n",
    "                         columns = feat_cols)], axis=1)\n",
    "        test_features = pd.concat([\n",
    "            test_features.drop(columns=feat_cols),\n",
    "            pd.DataFrame(qtrans.transform(test_features[feat_cols]),\n",
    "                         columns = feat_cols)], axis=1)\n",
    "\n",
    "    train = preprocess(train_features)\n",
    "    test = preprocess(test_features)\n",
    "    train_targets = train_targets.loc[train['cp_type'] == 0].reset_index(drop=True)\n",
    "    train = train.loc[train['cp_type'] == 0].reset_index(drop=True)\n",
    "    \n",
    "    train, test, feat_cols = pca_train_test(train, test, feat_cols, \n",
    "                                            g_cols, c_cols,\n",
    "                                            r_comps, seed)\n",
    "    train = get_feats_stats(train, g_cols, c_cols)\n",
    "    test = get_feats_stats(test, g_cols, c_cols)\n",
    "    feat_cols.extend([x for x in train.columns if 'stats_' in x])\n",
    "    print('features:', len(feat_cols))\n",
    "    \n",
    "    if pipe_scaler == 1:\n",
    "        scaler = RobustScaler()\n",
    "    elif pipe_scaler == 2:\n",
    "        scaler = MinMaxScaler()\n",
    "    elif pipe_scaler == 3:\n",
    "        scaler = StandardScaler()\n",
    "    train[feat_cols] = scaler.fit_transform(train[feat_cols])\n",
    "    test[feat_cols] = scaler.transform(test[feat_cols])\n",
    "    \n",
    "    threshold = train[feat_cols].var().sort_values().quantile(PARAMS['THRESHOLD'])\n",
    "    print('threshold {:.4f}'.format(threshold))\n",
    "    print('features total:', len(feat_cols))\n",
    "    var_thresh = VarianceThreshold(threshold)\n",
    "    train, feat_cols = var_thresh.fit_transform(train, feat_cols)\n",
    "    test = var_thresh.transform(test)\n",
    "    print(f'features total with variance threshold {threshold:.4f}:', len(feat_cols))\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f'time elapsed: {elapsed_time // 60:.0f} min {elapsed_time % 60:.0f} sec')\n",
    "    \n",
    "    return train, train_targets, test, feat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_columns, num_columns_rs=0, units=1024, drop=.4, lbl_smooth=.001, pipe=1):\n",
    "    if pipe == 0:\n",
    "        model = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Input(num_columns),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(drop),\n",
    "                tfa.layers.WeightNormalization(\n",
    "                    tf.keras.layers.Dense(units, activation='elu')\n",
    "                ),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(drop),\n",
    "                tfa.layers.WeightNormalization(\n",
    "                    tf.keras.layers.Dense(206, activation='sigmoid')\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    elif pipe == 1:\n",
    "        model = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Input(num_columns),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(drop),\n",
    "                tfa.layers.WeightNormalization(\n",
    "                    tf.keras.layers.Dense(units, activation='elu')\n",
    "                ),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(drop),\n",
    "                tfa.layers.WeightNormalization(\n",
    "                    tf.keras.layers.Dense(int(units / 2), activation='elu')\n",
    "                ),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(drop),\n",
    "                tfa.layers.WeightNormalization(\n",
    "                    tf.keras.layers.Dense(206, activation='sigmoid')\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    elif pipe == 2:\n",
    "        model = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Input(num_columns),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(drop),\n",
    "                tfa.layers.WeightNormalization(\n",
    "                    tf.keras.layers.Dense(units, activation='elu')\n",
    "                ),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(drop),\n",
    "                tfa.layers.WeightNormalization(\n",
    "                    tf.keras.layers.Dense(int(units / 2), activation='elu')\n",
    "                ),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(drop),\n",
    "                tfa.layers.WeightNormalization(\n",
    "                    tf.keras.layers.Dense(int(units / 4), activation='elu')\n",
    "                ),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(drop),\n",
    "                tfa.layers.WeightNormalization(\n",
    "                    tf.keras.layers.Dense(206, activation='sigmoid')\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    elif pipe == 3:\n",
    "        model = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Input(num_columns),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(drop / 2),\n",
    "                tfa.layers.WeightNormalization(\n",
    "                    tf.keras.layers.Dense(int(units / 2), activation='elu')\n",
    "                ),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(drop / 2),\n",
    "                tfa.layers.WeightNormalization(\n",
    "                    tf.keras.layers.Dense(int(units / 2), activation='elu')\n",
    "                ),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(drop / 2),\n",
    "                tfa.layers.WeightNormalization(\n",
    "                    tf.keras.layers.Dense(int(units / 2), activation='elu')\n",
    "                ),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(drop / 2),\n",
    "                tfa.layers.WeightNormalization(\n",
    "                    tf.keras.layers.Dense(int(units / 2), activation='elu')\n",
    "                ),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(drop / 2),\n",
    "                tfa.layers.WeightNormalization(\n",
    "                    tf.keras.layers.Dense(206, activation='sigmoid')\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        raise AttributeError('Cannot recover attribute for model pipe')\n",
    "    model.compile(\n",
    "        optimizer=tfa.optimizers.Lookahead(\n",
    "            tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "            sync_period=10\n",
    "        ),\n",
    "        loss=losses.BinaryCrossentropy(label_smoothing=lbl_smooth),\n",
    "        metrics=tf.keras.losses.BinaryCrossentropy(name='score')\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def metric(y_true, y_pred, smooth=.001):\n",
    "    metrics = []\n",
    "    y_pred = np.clip(y_pred, smooth, 1 - smooth)\n",
    "    for _target in y_true.columns:\n",
    "        metrics.append(\n",
    "            log_loss(\n",
    "                y_true.loc[:, _target], \n",
    "                y_pred.loc[:, _target].astype(float), \n",
    "                labels=[0, 1]\n",
    "            )\n",
    "        )\n",
    "    return np.mean(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogPrintingCallback(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.val_score = []\n",
    "        self.val_loss = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.val_score.append(logs['val_score'])\n",
    "        self.val_loss.append(logs['val_loss'])\n",
    "        if epoch % min(100, PARAMS['PATIENCE']) == 0 or epoch == (PARAMS['EPOCHS'] - 1):\n",
    "            print(\n",
    "                f\"epoch {epoch + 1} | loss: {logs['loss']:.5f} | score: {logs['score']}\",\n",
    "                f\"| val loss: {logs['val_loss']:.5f} | val score: {logs['val_score']}\"\n",
    "            )\n",
    "            \n",
    "    def on_train_end(self, lowest_val_loss, logs=None):\n",
    "        best_epoch = np.argmin(self.val_score) # np.argmin(self.val_loss)\n",
    "        best_score = self.val_score[best_epoch] # self.val_loss[best_epoch]\n",
    "        print(f'best model at epoch {best_epoch + 1} | score: {best_score}')\n",
    "        \n",
    "def get_lr_callback(batch_size=10, epochs=100, warmup=.025, plot=False):\n",
    "    lr_start = 1e-5\n",
    "    lr_max = 1e-2 #10 * lr_start * batch_size\n",
    "    lr_min = lr_start / 10\n",
    "    lr_ramp_ep = epochs * warmup\n",
    "    lr_sus_ep = 0\n",
    "    lr_decay = .95\n",
    "    \n",
    "    def lr_scheduler(epoch):\n",
    "        if epoch < lr_ramp_ep:\n",
    "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "            lr = lr_max\n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_decay ** (epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "        return lr\n",
    "        \n",
    "    if not plot:\n",
    "        lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=False)\n",
    "        return lr_callback \n",
    "    else: \n",
    "        return lr_scheduler\n",
    "    \n",
    "if PARAMS['DECAY']:\n",
    "    lr_scheduler_plot = get_lr_callback(\n",
    "        batch_size=PARAMS['BATCH_SIZE'], \n",
    "        epochs=PARAMS['EPOCHS'], \n",
    "        plot=True\n",
    "    )\n",
    "    xs = [i for i in range(PARAMS['EPOCHS'])]\n",
    "    y = [lr_scheduler_plot(x) for x in xs]\n",
    "    plt.plot(xs, y)\n",
    "    plt.title(f'lr schedule from {y[0]:.5f} to {max(y):.3f} to {y[-1]:.8f}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_shuffled(X, cols_to_shuffle=None, pre_shuffle=False, random_state=None):\n",
    "    rng = check_random_state(random_state)\n",
    "    if cols_to_shuffle is None:\n",
    "        cols_to_shuffle = range(X.shape[1])\n",
    "    if pre_shuffle:\n",
    "        X_shuffled = X.copy()\n",
    "        rng.shuffle(X_shuffled)\n",
    "    X_res = X.copy()\n",
    "    for col in tqdm(cols_to_shuffle):\n",
    "        if pre_shuffle:\n",
    "            X_res[:, col] = X_shuffled[:, col]\n",
    "        else:\n",
    "            rng.shuffle(X_res[:, col])\n",
    "        yield X_res\n",
    "        X_res[:, col] = X[:, col]\n",
    "\n",
    "def get_score_importances(score_func, X, y, n_iter=5, cols_to_shuffle=None, random_state=None):\n",
    "    rng = check_random_state(random_state)\n",
    "    base_score = score_func(X, y)\n",
    "    scores_decreases = []\n",
    "    for i in range(n_iter):\n",
    "        scores_shuffled = _get_scores_shufled(\n",
    "            score_func, \n",
    "            X, y, \n",
    "            cols_to_shuffle=cols_to_shuffle,\n",
    "            random_state=rng, \n",
    "            base_score=base_score\n",
    "        )\n",
    "        scores_decreases.append(scores_shuffled)\n",
    "    return base_score, scores_decreases\n",
    "\n",
    "def _get_scores_shufled(score_func, X, y, base_score, cols_to_shuffle=None, random_state=None):\n",
    "    Xs = iter_shuffled(X, cols_to_shuffle, random_state=random_state)\n",
    "    res = []\n",
    "    for X_shuffled in Xs:\n",
    "        res.append(-score_func(X_shuffled, y) + base_score)\n",
    "    return res\n",
    "\n",
    "def _metric(y_true, y_pred, smooth=.001):\n",
    "    metrics = []\n",
    "    y_pred = np.clip(y_pred, smooth, 1 - smooth)\n",
    "    for i in range(y_pred.shape[1]):\n",
    "        if y_true[:, i].sum() > 1:\n",
    "            metrics.append(\n",
    "                log_loss(\n",
    "                    y_true[:, i], \n",
    "                    y_pred[:, i].astype(float),\n",
    "                    labels=[0, 1]\n",
    "                )\n",
    "            )\n",
    "    return np.mean(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(mparams, X_trn, y_trn, X_val, y_val, n_fold, cols,\n",
    "                X_tst=None, seed=False):\n",
    "    model = get_model(\n",
    "            len(cols), \n",
    "            units=mparams['UNITS'], \n",
    "            drop=mparams['DROPOUT'], \n",
    "            lbl_smooth=mparams['LBL_SMOOTH'],\n",
    "            pipe=mparams['PIPE']\n",
    "    )\n",
    "    if seed:\n",
    "        checkpoint_path = f'{MODELS_PATH}/seed_{seed}_fold_{n_fold}.hdf5'\n",
    "    else:\n",
    "        checkpoint_path = f'{MODELS_PATH}/feat_imp_fold_{n_fold}.hdf5'\n",
    "    earlystopper = EarlyStopping(\n",
    "        monitor='val_score', \n",
    "        patience=mparams['PATIENCE'], \n",
    "        verbose=0,\n",
    "        mode='min'\n",
    "    )\n",
    "    lrreducer = ReduceLROnPlateau(\n",
    "        monitor='val_score', \n",
    "        factor=.1, \n",
    "        patience=int(mparams['PATIENCE'] / 2), \n",
    "        verbose=0, \n",
    "        min_lr=1e-5,\n",
    "        mode='min'\n",
    "    )\n",
    "    checkpointer = ModelCheckpoint(\n",
    "        checkpoint_path, \n",
    "        monitor='val_score', \n",
    "        verbose=0, \n",
    "        save_best_only=True,\n",
    "        save_weights_only=True, \n",
    "        mode='min'\n",
    "    )\n",
    "    callbacks = [earlystopper, checkpointer, LogPrintingCallback()]\n",
    "    if mparams['DECAY']:\n",
    "        callbacks.append(get_lr_callback(mparams['BATCH_SIZE']))\n",
    "        print('lr warmup and decay')\n",
    "    else:\n",
    "        callbacks.append(lrreducer)\n",
    "        print('lr reduce on plateau')\n",
    "    model.fit(\n",
    "        X_trn, y_trn,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=mparams['EPOCHS'], \n",
    "        batch_size=mparams['BATCH_SIZE'],\n",
    "        callbacks=callbacks, \n",
    "        verbose=0\n",
    "    )\n",
    "    if mparams['PSEUDO_LBL']:\n",
    "        print('-' * 5, 'pseudo label training', '-' * 5)\n",
    "        test_predict = model.predict(X_tst)\n",
    "        model = get_model(\n",
    "            len(cols), \n",
    "            units=mparams['UNITS'], \n",
    "            drop=mparams['DROPOUT'], \n",
    "            lbl_smooth=mparams['LBL_SMOOTH']\n",
    "        )\n",
    "        model.fit(\n",
    "            np.vstack([X_trn, X_tst]),\n",
    "            np.vstack([y_trn, test_predict]),\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=mparams['EPOCHS'], \n",
    "            batch_size=mparams['BATCH_SIZE'],\n",
    "            callbacks=callbacks, \n",
    "            verbose=0\n",
    "        )\n",
    "    model.load_weights(checkpoint_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PARAMS['FEAT_IMP']:\n",
    "    train, train_targets, test, _ = get_train_test(\n",
    "        train_features_raw, train_targets_raw, test_features_raw, \n",
    "        r_comps=PARAMS['REDUCE_COMPS'],\n",
    "        q_flag=PARAMS['QTRANS'], pipe_scaler=PARAMS['PIPE_SCALER'],\n",
    "        seed=PARAMS['SEED']\n",
    "    )\n",
    "    perm_imps = np.zeros(train.shape[1])\n",
    "    all_res = []\n",
    "    mskf = MultilabelStratifiedKFold(\n",
    "        n_splits=PARAMS['FOLDS'],     \n",
    "        random_state=PARAMS['SEED'],         \n",
    "        shuffle=True\n",
    "    ).split(train_targets, train_targets)\n",
    "    for n, (tr, te) in enumerate(mskf):\n",
    "        print('=' * 10, f'feature importances | FOLD {n}', '=' * 10)\n",
    "        model = train_model(\n",
    "            mparams=PARAMS, \n",
    "            X_trn=train.values[tr], \n",
    "            y_trn=train_targets.values[tr], \n",
    "            X_val=train.values[te], \n",
    "            y_val=train_targets.values[te], \n",
    "            n_fold=n, \n",
    "            cols=train.columns,\n",
    "            X_tst=None,\n",
    "            seed=False\n",
    "        )\n",
    "\n",
    "        def _score(X, y):\n",
    "            pred = model.predict(X)\n",
    "            return _metric(y, pred, smooth=PARAMS['LBL_SMOOTH'])\n",
    "\n",
    "        base_score, fold_imp = get_score_importances(\n",
    "            _score, \n",
    "            train.values[te], train_targets.values[te], \n",
    "            n_iter=PARAMS['FEAT_IMP'], \n",
    "            random_state=PARAMS['SEED']\n",
    "        )\n",
    "        all_res.append(fold_imp)\n",
    "        perm_imps += np.mean(fold_imp, axis=0)\n",
    "        print('')\n",
    "    top_feats = np.argwhere(perm_imps < 0).flatten()\n",
    "    print('found features:', len(top_feats))\n",
    "    with open(f'{MODELS_PATH}/top_feats.npy', 'wb') as file:\n",
    "        np.save(file, top_feats)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f'time elapsed: {elapsed_time // 60:.0f} min {elapsed_time % 60:.0f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "    #'FOLDS': hp.choice('FOLDS', [8]),\n",
    "    #'BATCH_SIZE': hp.choice('BATCH_SIZE', [32, 64, 128]),\n",
    "    #'DECAY': hp.choice('DECAY', [True]),\n",
    "    #'PATIENCE': hp.choice('PATIENCE', [10]),\n",
    "    #'UNITS': hp.choice('UNITS', [1024]),\n",
    "    'DROPOUT': hp.quniform('DROPOUT', .1, .5, .01),\n",
    "    'LBL_SMOOTH': hp.quniform('LBL_SMOOTH', .0001, .005, .00001),\n",
    "    'REDUCE_COMPS': hp.choice('REDUCE_COMPS', [20, 10]),\n",
    "    'THRESHOLD':  hp.quniform('THRESHOLD', 0, 5e-2, 1e-3),\n",
    "    'PIPE_SCALER':  hp.choice('PIPE_SCALER', [1, 2, 3]),\n",
    "    'QTRANS': hp.choice('QTRANS', [True, False]),\n",
    "}\n",
    "MAX_EVALS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PARAMS['HOPT']:\n",
    "    PARAMS_OPT = PARAMS.copy()\n",
    "\n",
    "    def objective(params):\n",
    "        print('=' * 50, '\\n', '=' * 50)\n",
    "        PARAMS_OPT.update(params)\n",
    "        train, train_targets, test, _ = get_train_test(\n",
    "            train_features_raw, train_targets_raw, test_features_raw, \n",
    "            r_comps=PARAMS_OPT['REDUCE_COMPS'],\n",
    "            q_flag=PARAMS_OPT['QTRANS'], pipe_scaler=PARAMS_OPT['PIPE_SCALER'],\n",
    "            seed=PARAMS_OPT['SEED']\n",
    "        )\n",
    "        top_feats = list(range(len(train.columns)))\n",
    "        print('top features:', len(top_feats))\n",
    "        res = train_targets.copy()\n",
    "        res.loc[:, train_targets.columns] = 0\n",
    "        mskf = MultilabelStratifiedKFold(\n",
    "            n_splits=PARAMS_OPT['FOLDS'],     \n",
    "            random_state=PARAMS_OPT['SEED'],\n",
    "            shuffle=True\n",
    "        ).split(train_targets, train_targets)\n",
    "        for n, (tr, te) in enumerate(mskf):\n",
    "            print('=' * 10, f'HYPEROPT | FOLD {n}', '=' * 10)\n",
    "            model = train_model(\n",
    "                mparams=PARAMS_OPT, \n",
    "                X_trn=train.values[tr][:, top_feats], \n",
    "                y_trn=train_targets.values[tr], \n",
    "                X_val=train.values[te][:, top_feats], \n",
    "                y_val=train_targets.values[te], \n",
    "                n_fold=n, \n",
    "                cols=top_feats,\n",
    "                X_tst=test.values[:, top_feats],\n",
    "                seed=PARAMS_OPT['SEED']\n",
    "            )\n",
    "            val_predict = model.predict(train.values[te][:, top_feats])\n",
    "            res.loc[te, train_targets.columns] += val_predict\n",
    "            print('')\n",
    "        oof_metric = metric(train_targets, res, smooth=PARAMS_OPT['LBL_SMOOTH'])\n",
    "        print(f'\\nparams: {params} | oof metric: {oof_metric}\\n')\n",
    "        return oof_metric\n",
    "\n",
    "    best_hopt = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=MAX_EVALS)\n",
    "    print('best search:', best_hopt, '\\nbest params:', space_eval(space, best_hopt))\n",
    "    PARAMS_OPT.update(space_eval(space, best_hopt))\n",
    "    params_file = f'{MODELS_PATH}/hopt_params.json'\n",
    "    with open(params_file, 'w') as file:\n",
    "        json.dump(PARAMS_OPT, file)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f'time elapsed: {elapsed_time // 60:.0f} min {elapsed_time % 60:.0f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not PARAMS['HOPT']:\n",
    "    res = train_targets.copy()\n",
    "    ssubm.loc[:, train_targets.columns] = 0\n",
    "    res.loc[:, train_targets.columns] = 0\n",
    "    for seed in range(PARAMS['SEEDS']):\n",
    "        mskf = MultilabelStratifiedKFold(\n",
    "            n_splits=PARAMS['FOLDS'],     \n",
    "            random_state=PARAMS['SEED'] + seed,\n",
    "            shuffle=True\n",
    "        ).split(train_targets, train_targets)\n",
    "        for n, (tr, te) in enumerate(mskf):\n",
    "            print('=' * 10, f'SEED {seed} | FOLD {n}', '=' * 10)\n",
    "            model = train_model(\n",
    "                mparams=PARAMS, \n",
    "                X_trn=train.values[tr][:, top_feats], \n",
    "                y_trn=train_targets.values[tr], \n",
    "                X_val=train.values[te][:, top_feats], \n",
    "                y_val=train_targets.values[te], \n",
    "                n_fold=n, \n",
    "                cols=top_feats,\n",
    "                X_tst=test.values[:, top_feats],\n",
    "                seed=seed\n",
    "            )\n",
    "            test_predict = model.predict(test.values[:, top_feats])\n",
    "            val_predict = model.predict(train.values[te][:, top_feats])\n",
    "            ssubm.loc[:, train_targets.columns] += test_predict\n",
    "            res.loc[te, train_targets.columns] += val_predict\n",
    "            print('')\n",
    "    ssubm.loc[:, train_targets.columns] /= (PARAMS['FOLDS'] * PARAMS['SEEDS'])\n",
    "    res.loc[:, train_targets.columns] /= PARAMS['SEEDS']\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f'time elapsed: {elapsed_time // 60:.0f} min {elapsed_time % 60:.0f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not PARAMS['HOPT']:\n",
    "    print(\n",
    "        'params:', PARAMS,\n",
    "        f'\\nOOF metric: {metric(train_targets, res, smooth=PARAMS[\"LBL_SMOOTH\"])}'\n",
    "    )\n",
    "    ssubm.loc[test['cp_type'] == 1, train_targets.columns] = 0\n",
    "    ssubm.to_csv('submission_.csv', index=False)\n",
    "    \n",
    "    save_dict = PARAMS\n",
    "    save_dict['OOF metric'] = metric(train_targets, res)\n",
    "    if not os.path.exists('results.csv'):\n",
    "        df_save = pd.DataFrame(save_dict, index=[0])\n",
    "        df_save.to_csv('results.csv', sep='\\t')\n",
    "    else:\n",
    "        df_old = pd.read_csv('results.csv', sep='\\t', index_col=0)\n",
    "        df_save = pd.DataFrame(save_dict, index=[df_old.index.max() + 1])\n",
    "        df_save = df_old.append(df_save, ignore_index=True)\n",
    "        df_save.to_csv('results.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('results.csv', sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Orange Python 3",
   "language": "python",
   "name": "orange"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
